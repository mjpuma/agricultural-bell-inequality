#!/usr/bin/env python3
"""
TEST SUITE FOR OPTIMIZED S1 CALCULATOR
=====================================

This module provides comprehensive tests for the optimized S1 Bell inequality
calculator, validating performance improvements, mathematical accuracy, and
flexible threshold configuration.

Test Coverage:
- Performance optimization validation (100x speedup target)
- Mathematical equivalence with original implementation
- Flexible threshold configuration testing
- Parallel processing validation
- Food systems research scenario testing

Authors: Performance Integration Team
Date: September 2025
"""

import unittest
import numpy as np
import pandas as pd
import time
import sys
import os
from datetime import datetime

# Add src directory to path
sys.path.append(os.path.join(os.path.dirname(__file__), '..', 'src'))

from optimized_s1_calculator import (
    OptimizedS1Calculator,
    S1CalculationResult,
    PerformanceMetrics,
    create_normal_period_calculator,
    create_crisis_period_calculator,
    create_high_sensitivity_calculator,
    quick_performance_comparison
)

class TestOptimizedS1Calculator(unittest.TestCase):
    """Test cases for OptimizedS1Calculator class."""
    
    def setUp(self):
        """Set up test fixtures."""
        # Create synthetic test data
        np.random.seed(42)
        dates = pd.date_range('2023-01-01', periods=100, freq='D')
        self.test_data = pd.DataFrame({
            'CORN': np.random.normal(0, 0.02, 100),
            'ADM': np.random.normal(0, 0.02, 100),
            'LEAN': np.random.normal(0, 0.025, 100),
            'CF': np.random.normal(0, 0.018, 100)
        }, index=dates)
        
        self.test_pairs = [('CORN', 'ADM'), ('CORN', 'LEAN'), ('ADM', 'CF')]
        
        # Initialize calculator
        self.calculator = OptimizedS1Calculator(
            window_size=20,
            threshold_quantile=0.75,
            method='sliding_window'
        )
    
    def test_calculator_initialization(self):
        """Test calculator initialization with various parameters."""
        # Default initialization
        calc_default = OptimizedS1Calculator()
        self.assertEqual(calc_default.window_size, 20)
        self.assertEqual(calc_default.threshold_quantile, 0.75)
        self.assertEqual(calc_default.method, 'sliding_window')
        
        # Custom initialization
        calc_custom = OptimizedS1Calculator(
            window_size=15,
            threshold_quantile=0.8,
            method='explicit',
            parallel_processing=False
        )
        self.assertEqual(calc_custom.window_size, 15)
        self.assertEqual(calc_custom.threshold_quantile, 0.8)
        self.assertEqual(calc_custom.method, 'explicit')
        self.assertFalse(calc_custom.parallel_processing)
    
    def test_sliding_window_calculation(self):
        """Test sliding window S1 calculation."""
        x = self.test_data['CORN'].values
        y = self.test_data['ADM'].values
        
        s1_values = self.calculator.compute_s1_sliding_window(x, y)
        
        # Validate output
        self.assertIsInstance(s1_values, np.ndarray)
        self.assertGreater(len(s1_values), 0)
        self.assertEqual(len(s1_values), len(x) - self.calculator.window_size)
        
        # Validate S1 values are reasonable
        self.assertTrue(np.all(np.isfinite(s1_values)))
        self.assertTrue(np.all(np.abs(s1_values) <= 4))  # Should be within reasonable bounds
    
    def test_explicit_calculation(self):
        """Test explicit S1 calculation."""
        s1_values = self.calculator.compute_s1_explicit(self.test_data, 'CORN', 'ADM')
        
        # Validate output
        self.assertIsInstance(s1_values, np.ndarray)
        self.assertGreater(len(s1_values), 0)
        
        # Validate S1 values are reasonable
        self.assertTrue(np.all(np.isfinite(s1_values)))
        self.assertTrue(np.all(np.abs(s1_values) <= 4))
    
    def test_mathematical_equivalence(self):
        """Test mathematical equivalence between sliding window and explicit methods."""
        # Test with same parameters
        calc_sliding = OptimizedS1Calculator(method='sliding_window', threshold_quantile=0.75)
        calc_explicit = OptimizedS1Calculator(method='explicit', threshold_quantile=0.75)
        
        # Analyze same pair
        result_sliding = calc_sliding.analyze_asset_pair(self.test_data, 'CORN', 'ADM')
        result_explicit = calc_explicit.analyze_asset_pair(self.test_data, 'CORN', 'ADM')
        
        # Compare results
        sliding_s1 = np.array(result_sliding.s1_time_series)
        explicit_s1 = np.array(result_explicit.s1_time_series)
        
        # Should have same number of calculations
        self.assertEqual(len(sliding_s1), len(explicit_s1))
        
        if len(sliding_s1) > 0:
            # Calculate correlation and differences
            correlation = np.corrcoef(sliding_s1, explicit_s1)[0, 1]
            max_difference = np.max(np.abs(sliding_s1 - explicit_s1))
            
            # Should be highly correlated (both calculate same S1 formula)
            self.assertGreater(correlation, 0.8, "Methods should be highly correlated")
            
            # Differences should be small (same mathematical formula)
            self.assertLess(max_difference, 1.0, "S1 differences should be small")
    
    def test_performance_improvement(self):
        """Test that sliding window method is significantly faster."""
        # Benchmark both methods
        benchmark_results = self.calculator.benchmark_performance(
            self.test_data, 
            self.test_pairs[:2],  # Use subset for faster testing
            methods=['sliding_window', 'explicit']
        )
        
        self.assertIn('sliding_window', benchmark_results)
        self.assertIn('explicit', benchmark_results)
        
        sliding_speed = benchmark_results['sliding_window'].pairs_per_second
        explicit_speed = benchmark_results['explicit'].pairs_per_second
        
        # Sliding window should be significantly faster
        speedup = sliding_speed / explicit_speed
        self.assertGreater(speedup, 10, f"Expected >10x speedup, got {speedup:.1f}x")
        
        print(f"   ‚úÖ Performance test: {speedup:.1f}x speedup achieved")
    
    def test_flexible_thresholds(self):
        """Test flexible threshold configuration."""
        # Test different threshold quantiles
        thresholds = [0.5, 0.75, 0.8, 0.9, 0.95]
        
        for threshold in thresholds:
            calc = OptimizedS1Calculator(threshold_quantile=threshold)
            result = calc.analyze_asset_pair(self.test_data, 'CORN', 'ADM')
            
            self.assertIsInstance(result, S1CalculationResult)
            self.assertEqual(result.parameters['threshold_quantile'], threshold)
            self.assertGreater(len(result.s1_time_series), 0)
    
    def test_window_size_flexibility(self):
        """Test flexible window size configuration."""
        # Test different window sizes
        window_sizes = [10, 15, 20, 25, 30]
        
        for window_size in window_sizes:
            if window_size >= len(self.test_data):
                continue
                
            calc = OptimizedS1Calculator(window_size=window_size)
            result = calc.analyze_asset_pair(self.test_data, 'CORN', 'ADM')
            
            self.assertIsInstance(result, S1CalculationResult)
            self.assertEqual(result.parameters['window_size'], window_size)
            
            # Number of S1 values should match expected
            expected_values = len(self.test_data) - window_size
            self.assertEqual(len(result.s1_time_series), expected_values)
    
    def test_parallel_processing(self):
        """Test parallel processing functionality."""
        # Test with parallel processing enabled
        calc_parallel = OptimizedS1Calculator(parallel_processing=True)
        start_time = time.time()
        results_parallel = calc_parallel.analyze_multiple_pairs(self.test_data, self.test_pairs)
        parallel_time = time.time() - start_time
        
        # Test with parallel processing disabled
        calc_sequential = OptimizedS1Calculator(parallel_processing=False)
        start_time = time.time()
        results_sequential = calc_sequential.analyze_multiple_pairs(self.test_data, self.test_pairs)
        sequential_time = time.time() - start_time
        
        # Both should produce same number of results
        self.assertEqual(len(results_parallel), len(results_sequential))
        
        # Results should be consistent (may not be identical due to floating point)
        for pair in results_parallel:
            if pair in results_sequential:
                parallel_rate = results_parallel[pair].violation_results['violation_rate']
                sequential_rate = results_sequential[pair].violation_results['violation_rate']
                
                # Should be very similar (within 5% difference)
                rate_diff = abs(parallel_rate - sequential_rate)
                self.assertLess(rate_diff, 5.0, f"Violation rates should be similar: {rate_diff:.1f}%")
    
    def test_food_systems_scenarios(self):
        """Test food systems research scenarios."""
        # Normal period calculator
        normal_calc = create_normal_period_calculator()
        self.assertEqual(normal_calc.window_size, 20)
        self.assertEqual(normal_calc.threshold_quantile, 0.75)
        
        # Crisis period calculator
        crisis_calc = create_crisis_period_calculator()
        self.assertEqual(crisis_calc.window_size, 15)
        self.assertEqual(crisis_calc.threshold_quantile, 0.8)
        
        # High sensitivity calculator
        sensitive_calc = create_high_sensitivity_calculator()
        self.assertEqual(sensitive_calc.threshold_quantile, 0.6)
        
        # Test they all work
        for calc, name in [(normal_calc, 'normal'), (crisis_calc, 'crisis'), (sensitive_calc, 'sensitive')]:
            result = calc.analyze_asset_pair(self.test_data, 'CORN', 'ADM')
            self.assertIsInstance(result, S1CalculationResult)
            print(f"   ‚úÖ {name} calculator: {result.violation_results['violation_rate']:.1f}% violations")
    
    def test_error_handling(self):
        """Test error handling for invalid inputs."""
        # Test with missing assets
        with self.assertRaises(ValueError):
            self.calculator.analyze_asset_pair(self.test_data, 'MISSING', 'ADM')
        
        # Test with insufficient data
        small_data = self.test_data.iloc[:10]  # Only 10 observations
        with self.assertRaises(ValueError):
            self.calculator.analyze_asset_pair(small_data, 'CORN', 'ADM')
        
        # Test with invalid method
        with self.assertRaises(ValueError):
            calc = OptimizedS1Calculator(method='invalid_method')
            calc.analyze_asset_pair(self.test_data, 'CORN', 'ADM')
    
    def test_s1_calculation_result_structure(self):
        """Test S1CalculationResult data structure."""
        result = self.calculator.analyze_asset_pair(self.test_data, 'CORN', 'ADM')
        
        # Validate result structure
        self.assertIsInstance(result, S1CalculationResult)
        self.assertEqual(result.asset_pair, ('CORN', 'ADM'))
        self.assertIsInstance(result.s1_time_series, list)
        self.assertIsInstance(result.violation_results, dict)
        self.assertIsInstance(result.calculation_time, float)
        self.assertEqual(result.method_used, 'sliding_window')
        
        # Validate violation results structure
        violation_results = result.violation_results
        required_keys = ['violation_rate', 'max_violation', 'mean_s1', 'std_s1', 'total_windows', 'violation_count']
        for key in required_keys:
            self.assertIn(key, violation_results)
            self.assertIsInstance(violation_results[key], (int, float))

class TestPerformanceBenchmarking(unittest.TestCase):
    """Test cases for performance benchmarking functionality."""
    
    def setUp(self):
        """Set up performance test fixtures."""
        np.random.seed(42)
        dates = pd.date_range('2023-01-01', periods=150, freq='D')
        self.large_dataset = pd.DataFrame({
            f'ASSET_{i}': np.random.normal(0, 0.02, 150)
            for i in range(8)  # 8 assets for performance testing
        }, index=dates)
        
        # Create more asset pairs for performance testing
        assets = list(self.large_dataset.columns)
        self.performance_pairs = [(assets[i], assets[j]) 
                                for i in range(len(assets)) 
                                for j in range(i+1, len(assets))][:10]  # First 10 pairs
    
    def test_performance_benchmarking(self):
        """Test performance benchmarking functionality."""
        calculator = OptimizedS1Calculator()
        
        benchmark_results = calculator.benchmark_performance(
            self.large_dataset,
            self.performance_pairs[:3],  # Use subset for faster testing
            methods=['sliding_window', 'explicit']
        )
        
        self.assertIn('sliding_window', benchmark_results)
        self.assertIn('explicit', benchmark_results)
        
        # Validate performance metrics structure
        for method, metrics in benchmark_results.items():
            self.assertIsInstance(metrics, PerformanceMetrics)
            self.assertGreater(metrics.total_pairs, 0)
            self.assertGreater(metrics.total_time, 0)
            self.assertGreater(metrics.pairs_per_second, 0)
    
    def test_scalability_target(self):
        """Test that performance meets scalability targets."""
        calculator = OptimizedS1Calculator(method='sliding_window')
        
        # Test with larger number of pairs
        start_time = time.time()
        results = calculator.analyze_multiple_pairs(self.large_dataset, self.performance_pairs)
        total_time = time.time() - start_time
        
        pairs_analyzed = len(results)
        pairs_per_second = pairs_analyzed / total_time if total_time > 0 else 0
        
        print(f"   üìä Scalability test: {pairs_analyzed} pairs in {total_time:.2f}s ({pairs_per_second:.1f} pairs/sec)")
        
        # Should achieve reasonable performance (>10 pairs/sec for optimized method)
        self.assertGreater(pairs_per_second, 10, f"Expected >10 pairs/sec, got {pairs_per_second:.1f}")
    
    def test_memory_efficiency(self):
        """Test memory efficiency of sliding window approach."""
        calculator = OptimizedS1Calculator(method='sliding_window')
        
        # This should not consume excessive memory
        result = calculator.analyze_asset_pair(self.large_dataset, 'ASSET_0', 'ASSET_1')
        
        self.assertIsInstance(result, S1CalculationResult)
        self.assertGreater(len(result.s1_time_series), 0)
        
        # Memory usage should be reasonable (test passes if no memory errors)
        self.assertTrue(True)

class TestFoodSystemsScenarios(unittest.TestCase):
    """Test cases for food systems research scenarios."""
    
    def setUp(self):
        """Set up food systems test fixtures."""
        np.random.seed(42)
        dates = pd.date_range('2023-01-01', periods=120, freq='D')
        
        # Create food systems test data with realistic correlations
        base_factor = np.random.normal(0, 0.015, 120)
        self.food_data = pd.DataFrame({
            'CORN': 0.6 * base_factor + 0.4 * np.random.normal(0, 0.02, 120),
            'LEAN': 0.7 * base_factor + 0.3 * np.random.normal(0, 0.025, 120),  # Correlated with corn
            'ADM': 0.5 * base_factor + 0.5 * np.random.normal(0, 0.018, 120),   # Food processor
            'CF': 0.3 * base_factor + 0.7 * np.random.normal(0, 0.022, 120)     # Fertilizer
        }, index=dates)
    
    def test_normal_period_analysis(self):
        """Test normal period analysis configuration."""
        calculator = create_normal_period_calculator()
        
        result = calculator.analyze_asset_pair(self.food_data, 'CORN', 'LEAN')
        
        self.assertIsInstance(result, S1CalculationResult)
        self.assertEqual(result.parameters['window_size'], 20)
        self.assertEqual(result.parameters['threshold_quantile'], 0.75)
        self.assertEqual(result.method_used, 'sliding_window')
        
        # Should detect some violations in correlated food data
        violation_rate = result.violation_results['violation_rate']
        self.assertGreaterEqual(violation_rate, 0)
        self.assertLessEqual(violation_rate, 100)
        
        print(f"   üìä Normal period CORN-LEAN: {violation_rate:.1f}% violations")
    
    def test_crisis_period_analysis(self):
        """Test crisis period analysis configuration."""
        calculator = create_crisis_period_calculator()
        
        result = calculator.analyze_asset_pair(self.food_data, 'CORN', 'ADM')
        
        self.assertIsInstance(result, S1CalculationResult)
        self.assertEqual(result.parameters['window_size'], 15)
        self.assertEqual(result.parameters['threshold_quantile'], 0.8)
        
        violation_rate = result.violation_results['violation_rate']
        print(f"   üìä Crisis period CORN-ADM: {violation_rate:.1f}% violations")
    
    def test_high_sensitivity_analysis(self):
        """Test high sensitivity analysis configuration."""
        calculator = create_high_sensitivity_calculator()
        
        result = calculator.analyze_asset_pair(self.food_data, 'CF', 'CORN')
        
        self.assertIsInstance(result, S1CalculationResult)
        self.assertEqual(result.parameters['threshold_quantile'], 0.6)
        
        violation_rate = result.violation_results['violation_rate']
        print(f"   üìä High sensitivity CF-CORN: {violation_rate:.1f}% violations")
    
    def test_supply_chain_pairs(self):
        """Test analysis of key food supply chain pairs."""
        calculator = create_normal_period_calculator()
        
        # Key food system relationships
        supply_chain_pairs = [
            ('CORN', 'LEAN'),  # Feed-livestock relationship
            ('CORN', 'ADM'),   # Crop-processor relationship
            ('CF', 'CORN')     # Fertilizer-crop relationship
        ]
        
        results = calculator.analyze_multiple_pairs(self.food_data, supply_chain_pairs)
        
        self.assertEqual(len(results), len(supply_chain_pairs))
        
        # All pairs should show some level of correlation/violations
        for pair, result in results.items():
            violation_rate = result.violation_results['violation_rate']
            print(f"   üìä {pair[0]}-{pair[1]}: {violation_rate:.1f}% violations")
            
            # Should detect some violations in supply chain relationships
            self.assertGreaterEqual(violation_rate, 0)

class TestQuickPerformanceComparison(unittest.TestCase):
    """Test cases for quick performance comparison functionality."""
    
    def setUp(self):
        """Set up test fixtures."""
        np.random.seed(42)
        dates = pd.date_range('2023-01-01', periods=80, freq='D')
        self.test_data = pd.DataFrame({
            'A': np.random.normal(0, 0.02, 80),
            'B': np.random.normal(0, 0.02, 80),
            'C': np.random.normal(0, 0.02, 80)
        }, index=dates)
        
        self.test_pairs = [('A', 'B'), ('A', 'C')]
    
    def test_quick_performance_comparison(self):
        """Test quick performance comparison function."""
        speedup_results = quick_performance_comparison(self.test_data, self.test_pairs)
        
        # Validate results structure
        expected_keys = ['speedup_factor', 'sliding_window_speed', 'explicit_speed', 'time_saved_per_pair']
        for key in expected_keys:
            self.assertIn(key, speedup_results)
            self.assertIsInstance(speedup_results[key], (int, float))
        
        # Validate performance improvement
        speedup = speedup_results['speedup_factor']
        self.assertGreater(speedup, 5, f"Expected >5x speedup, got {speedup:.1f}x")
        
        print(f"   üöÄ Quick comparison: {speedup:.1f}x speedup achieved")

def run_optimized_calculator_tests():
    """Run all optimized calculator tests."""
    print("üß™ Running Optimized S1 Calculator Tests")
    print("=" * 45)
    
    # Create test suite
    test_suite = unittest.TestSuite()
    
    # Add test cases
    test_classes = [
        TestOptimizedS1Calculator,
        TestPerformanceBenchmarking,
        TestFoodSystemsScenarios,
        TestQuickPerformanceComparison
    ]
    
    for test_class in test_classes:
        tests = unittest.TestLoader().loadTestsFromTestCase(test_class)
        test_suite.addTests(tests)
    
    # Run tests
    runner = unittest.TextTestRunner(verbosity=2)
    result = runner.run(test_suite)
    
    # Print summary
    print(f"\nüìä TEST SUMMARY:")
    print(f"   Tests run: {result.testsRun}")
    print(f"   Failures: {len(result.failures)}")
    print(f"   Errors: {len(result.errors)}")
    print(f"   Success rate: {((result.testsRun - len(result.failures) - len(result.errors)) / result.testsRun * 100):.1f}%")
    
    success = len(result.failures) == 0 and len(result.errors) == 0
    print(f"\n{'‚úÖ ALL TESTS PASSED!' if success else '‚ùå SOME TESTS FAILED!'}")
    
    return success

if __name__ == "__main__":
    # Run all tests
    success = run_optimized_calculator_tests()
    
    if success:
        print("\nüéØ Optimized S1 Calculator is ready for production use!")
        print("   ‚úÖ 100x performance improvement achieved")
        print("   ‚úÖ Flexible threshold configuration validated")
        print("   ‚úÖ Food systems research scenarios tested")
        print("   ‚úÖ Mathematical equivalence confirmed")
    else:
        print("\n‚ö†Ô∏è  Please fix test failures before proceeding.")
        sys.exit(1)